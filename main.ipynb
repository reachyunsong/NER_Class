{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u1158286\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\u1158286\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from numpy import nan\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "from data_loader import Data_Loader\n",
    "from process_data import propress_data\n",
    "from process_data import embedding\n",
    "from model import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 train_loader,\n",
    "                #  dev_loader=None,\n",
    "                 test_loader,\n",
    "                 model,\n",
    "                 output_dir=None,\n",
    "                 optimizer=Adam(learning_rate=3e-5),\n",
    "                 epochs=2,\n",
    "                 device=\"cpu\",\n",
    "                 best_val_loss = float(\"inf\"),\n",
    "                 PATIENCE = 3\n",
    "                ):\n",
    "        self.output_dir = output_dir\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        # self.dev_loader = dev_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.best_val_loss = best_val_loss\n",
    "        self.PATIENCE = PATIENCE \n",
    "        \n",
    "     \n",
    "    def train(self, input_ids, attention_masks, label_ids):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model((input_ids, attention_masks), training=True)\n",
    "            seq_lengths = tf.reduce_sum(attention_masks, axis=1)\n",
    "            loss = self.model.compute_loss(logits, label_ids, seq_lengths)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def val(self, input_ids, attention_masks, label_ids):\n",
    "        logits = self.model((input_ids, attention_masks), training=False)\n",
    "        seq_lengths = tf.reduce_sum(attention_masks, axis=1)\n",
    "        loss = self.model.compute_loss(logits, label_ids, seq_lengths)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train_process(self):\n",
    "        train_dataset = self.train_loader\n",
    "        val_dataset = self.test_loader\n",
    "        \n",
    "        checkpoint = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n",
    "        checkpoint_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=1)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "            train_loss_avg = tf.keras.metrics.Mean()\n",
    "            val_loss_avg = tf.keras.metrics.Mean()\n",
    "            # 训练步骤\n",
    "            for batch_input_ids, batch_attention_masks, batch_label_ids in train_dataset:\n",
    "                loss = self.train(batch_input_ids, batch_attention_masks, batch_label_ids)\n",
    "                train_loss_avg.update_state(loss)\n",
    "            # 验证步骤\n",
    "            for val_input_ids_batch, val_attention_masks_batch, val_label_ids_batch in val_dataset:\n",
    "                val_loss = self.val(val_input_ids_batch, val_attention_masks_batch, val_label_ids_batch)\n",
    "                val_loss_avg.update_state(val_loss)\n",
    "            # 获取 epoch 损失\n",
    "            train_loss = train_loss_avg.result()\n",
    "            val_loss = val_loss_avg.result()\n",
    "            print(f\"Train Loss: {train_loss.numpy()}, Validation Loss: {val_loss.numpy()}\")\n",
    "            # Early Stopping 检查\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                checkpoint_manager.save()\n",
    "                print(f\"New best model saved with validation loss: {self.best_val_loss.numpy()}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"Patience Counter: {patience_counter}\")\n",
    "            if patience_counter >= self.PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "            print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path):\n",
    "    df = pd.read_csv(data_path,encoding= 'unicode_escape')\n",
    "    df = df[:1000]\n",
    "    X_train,X_test,y_train,y_test = propress_data(df).split_train_test()\n",
    "    input_ids_train,attention_mask_train, label_ids_train = embedding().tokenize(X_train,y_train)\n",
    "    input_ids_test,attention_mask_test, label_ids_test = embedding().tokenize(X_test,y_test)\n",
    "    train_loader, test_loader = Data_Loader().data_loader(input_ids_train,attention_mask_train, label_ids_train,input_ids_test,attention_mask_test, label_ids_test)\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(r'C:\\Users\\u1158286\\OneDrive - IQVIA\\Desktop\\NER\\bert_model\\bert-base-uncased',from_pt = True)\n",
    "    ner_model = NERModel(bert_model,16)\n",
    "\n",
    "    \n",
    "    Trainer(train_loader,test_loader,ner_model).train_process()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<string>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 34/34 [00:00<00:00, 68.02it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 90.21it/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Train Loss: 9.411261558532715, Validation Loss: 10.401260375976562\n",
      "New best model saved with validation loss: 10.401260375976562\n",
      "训练完成！\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Train Loss: 8.880681991577148, Validation Loss: 10.124919891357422\n",
      "New best model saved with validation loss: 10.124919891357422\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_path = r'C:\\Users\\u1158286\\OneDrive - IQVIA\\Desktop\\NER\\data\\ner_dataset.csv'\n",
    "    main(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "465785cf6d15a5a05be77e46eb1c343d845a5590712fcaa847c9c16479499c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
