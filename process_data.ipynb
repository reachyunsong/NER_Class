{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u1158286\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from numpy import nan\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class propress_data():\n",
    "    def __init__(self, data, test_size_ratio=None):\n",
    "        super().__init__()  ## 通过super方法调用继承父类的属性\n",
    "        self.data = data\n",
    "        \n",
    "        if test_size_ratio is not None:\n",
    "            self.test_size_ratio = test_size_ratio\n",
    "        else:\n",
    "            self.test_size_ratio = 0.2\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        self.data['Sentence #'] = self.data['Sentence #'].ffill()\n",
    "        data = self.data[['Sentence #','Word','Tag']]\n",
    "        data.rename(columns={'Sentence #':'Sentence','Word':'Token','Tag':'POS'},inplace=True)\n",
    "        return data\n",
    "        \n",
    "    def over_sampling(self):\n",
    "        data = self.preprocess_data().copy()\n",
    "        lst = [data]\n",
    "        max_size = 1356\n",
    "        for class_index, group in data.groupby('POS'):\n",
    "                if len(group)< max_size:\n",
    "                    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "        train_sample = pd.concat(lst)\n",
    "        train_sample = train_sample.sort_values(by='Sentence')\n",
    "        return train_sample\n",
    "    \n",
    "    # covert label(string) to int\n",
    "    # e.g. \"O\" ---> 16\n",
    "    def process_data(self):\n",
    "        data = self.over_sampling()\n",
    "        enc_pos = preprocessing.LabelEncoder()\n",
    "        data.loc[:, \"POS\"] = enc_pos.fit_transform(data[\"POS\"])\n",
    "        sentences = data.groupby(\"Sentence\")[\"Token\"].apply(list).values\n",
    "        pos = data.groupby(\"Sentence\")[\"POS\"].apply(list).values\n",
    "        return sentences, pos, enc_pos\n",
    "    \n",
    "    \n",
    "    def split_train_test(self):\n",
    "        data = self.process_data()[0]\n",
    "        label = self.process_data()[1]\n",
    "        X_train,X_test,y_train,y_test = train_test_split(data,label,random_state=42,test_size = self.test_size_ratio)\n",
    "        return X_train,X_test,y_train,y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding():\n",
    "    def __init__(self):\n",
    "        super().__init__()  ## 通过super方法调用继承父类的属性\n",
    "        self.MAX_LEN = 128\n",
    "    \n",
    "\n",
    "\n",
    "    def tokenize(self,data,labels):\n",
    "        # Using BERT Auto-Tokenizer to embedding\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        input_ids, attention_masks, label_ids = [], [], []\n",
    "        \n",
    "        for i in tqdm(range(len(data))):\n",
    "\n",
    "            ## drop nan\n",
    "            data[i] =[x for x in data[i] if pd.notnull(x)]\n",
    "            tokenized = tokenizer(\n",
    "                                    data[i],\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=True,\n",
    "                                    max_length=self.MAX_LEN,\n",
    "                                    return_tensors=\"tf\",\n",
    "                                )\n",
    "             \n",
    "            input_ids.append(tokenized[\"input_ids\"][0])\n",
    "            attention_masks.append(tokenized[\"attention_mask\"][0])\n",
    "\n",
    "\n",
    "            if len(labels[i]) > self.MAX_LEN:\n",
    "                label_ids.append(labels[i][:self.MAX_LEN])\n",
    "                \n",
    "            # Padding标签序列\n",
    "            else:\n",
    "                label_ids.append(\n",
    "                    labels[i] + [0] * (self.MAX_LEN - len(labels[i]))  # 0为填充标签\n",
    "                )\n",
    "        return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_masks), tf.convert_to_tensor(label_ids)\n",
    "        # return input_ids,attention_masks,label_ids\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "# from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import numpy as np\n",
    "# # 参数设置\n",
    "# # MAX_SEQ_LENGTH = 128\n",
    "# NUM_LABELS = 16  # 假设有10个标签类别\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 2\n",
    "# PATIENCE = 3  # Early Stopping 的耐心值\n",
    "# # 加载BERT分词器和模型\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = TFBertModel.from_pretrained(r'C:\\Users\\u1158286\\OneDrive - IQVIA\\Desktop\\NER\\model\\bert-base-uncased',from_pt = True)\n",
    "\n",
    "\n",
    "# # 使用 tf.data.Dataset 创建批次数据集\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#    (input_ids_train, attention_mask_train, label_ids_train)\n",
    "# ).shuffle(buffer_size=100).batch(BATCH_SIZE)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#    (input_ids_test, attention_mask_test, label_ids_test)\n",
    "# ).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 定义NER模型\n",
    "# class NERModel(tf.keras.Model):\n",
    "#    def __init__(self, bert_model, num_labels):\n",
    "#        super(NERModel, self).__init__()\n",
    "#        self.bert = bert_model\n",
    "#        self.bilstm = Bidirectional(LSTM(128, return_sequences=True))\n",
    "#        self.classifier = Dense(num_labels)\n",
    "#        self.transition_params = tf.Variable(\n",
    "#            tf.random.uniform(shape=(num_labels, num_labels))\n",
    "#        )\n",
    "#    def call(self, inputs, training=False):\n",
    "#        input_ids, attention_mask = inputs\n",
    "#        bert_output = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "#        lstm_output = self.bilstm(bert_output)\n",
    "#        logits = self.classifier(lstm_output)\n",
    "#        return logits\n",
    "#    def compute_loss(self, logits, labels, seq_lengths):\n",
    "#        log_likelihood, self.transition_params = crf_log_likelihood(\n",
    "#            logits, labels, seq_lengths, self.transition_params\n",
    "#        )\n",
    "#        return -tf.reduce_mean(log_likelihood)\n",
    "# # 创建模型实例\n",
    "# ner_model = NERModel(bert_model, NUM_LABELS)\n",
    "# optimizer = Adam(learning_rate=3e-5)\n",
    "# # 定义 Early Stopping 和 Checkpoint\n",
    "# best_val_loss = float(\"inf\")\n",
    "# patience_counter = 0  # Early Stopping 计数器\n",
    "# checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=ner_model)\n",
    "# checkpoint_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=1)\n",
    "# @tf.function\n",
    "# def train_step(input_ids, attention_masks, label_ids):\n",
    "#    with tf.GradientTape() as tape:\n",
    "#        logits = ner_model((input_ids, attention_masks), training=True)\n",
    "#        seq_lengths = tf.reduce_sum(attention_masks, axis=1)\n",
    "#        loss = ner_model.compute_loss(logits, label_ids, seq_lengths)\n",
    "#    gradients = tape.gradient(loss, ner_model.trainable_variables)\n",
    "#    optimizer.apply_gradients(zip(gradients, ner_model.trainable_variables))\n",
    "#    return loss\n",
    "# @tf.function\n",
    "# def val_step(input_ids, attention_masks, label_ids):\n",
    "#    logits = ner_model((input_ids, attention_masks), training=False)\n",
    "#    seq_lengths = tf.reduce_sum(attention_masks, axis=1)\n",
    "#    loss = ner_model.compute_loss(logits, label_ids, seq_lengths)\n",
    "#    return loss\n",
    "# # 训练循环\n",
    "# for epoch in range(EPOCHS):\n",
    "#    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "#    train_loss_avg = tf.keras.metrics.Mean()\n",
    "#    val_loss_avg = tf.keras.metrics.Mean()\n",
    "#    # 训练步骤\n",
    "#    for batch_input_ids, batch_attention_masks, batch_label_ids in train_dataset:\n",
    "#        loss = train_step(batch_input_ids, batch_attention_masks, batch_label_ids)\n",
    "#        train_loss_avg.update_state(loss)\n",
    "#    # 验证步骤\n",
    "#    for val_input_ids_batch, val_attention_masks_batch, val_label_ids_batch in val_dataset:\n",
    "#        val_loss = val_step(val_input_ids_batch, val_attention_masks_batch, val_label_ids_batch)\n",
    "#        val_loss_avg.update_state(val_loss)\n",
    "#    # 获取 epoch 损失\n",
    "#    train_loss = train_loss_avg.result()\n",
    "#    val_loss = val_loss_avg.result()\n",
    "#    print(f\"Train Loss: {train_loss.numpy()}, Validation Loss: {val_loss.numpy()}\")\n",
    "#    # Early Stopping 检查\n",
    "#    if val_loss < best_val_loss:\n",
    "#        best_val_loss = val_loss\n",
    "#        patience_counter = 0\n",
    "#        checkpoint_manager.save()\n",
    "#        print(f\"New best model saved with validation loss: {best_val_loss.numpy()}\")\n",
    "#    else:\n",
    "#        patience_counter += 1\n",
    "#        print(f\"Patience Counter: {patience_counter}\")\n",
    "#    if patience_counter >= PATIENCE:\n",
    "#        print(\"Early stopping triggered.\")\n",
    "#        break\n",
    "# print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "465785cf6d15a5a05be77e46eb1c343d845a5590712fcaa847c9c16479499c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
